{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf965f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Any, Optional, Union, Callable\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sqlparse\n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT_PATH = '/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/'\n",
    "\n",
    "import sys\n",
    "sys.path.append(ROOT_PATH + 'DataSampling/src/models/pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03931798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_api_key(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads the API key from a file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"API key file not found: {file_path}\")\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        api_key = file.read().strip()\n",
    "    \n",
    "    if not api_key:\n",
    "        raise ValueError(\"API key is empty.\")\n",
    "    \n",
    "    return api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae1e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIRD_DATA_PATH = ROOT_PATH + 'DataSampling/data/enriched_dataset/v2/bird_set_stratified'\n",
    "SPIDER_DATA_PATH = ROOT_PATH + 'DataSampling/data/enriched_dataset/v2/spider_set_stratified'\n",
    "SPIDER2_DATA_PATH = ROOT_PATH + 'DataSampling/data/enriched_dataset/v2/spider2_lite_set'\n",
    "\n",
    "model_configs = [\n",
    "    # Anthropic Claude with extended thinking\n",
    "    {\n",
    "        \"type\": \"anthropic\",\n",
    "        \"name\": \"claude-3-7-sonnet-20250219\",\n",
    "        \"api_key\": read_api_key(ROOT_PATH + 'Data/Auth/anthropic.api.key/text2sql.key'),\n",
    "        \"extended_thinking\": True,\n",
    "    },\n",
    "    # Together.ai model\n",
    "    {\n",
    "        \"type\": \"together_ai\",\n",
    "        \"name\": \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "        \"api_key\": read_api_key(ROOT_PATH + 'Data/Auth/together.ai.api.key/API.key'),\n",
    "    },\n",
    "    # Together.ai model DeepSeek R1 \n",
    "    {\n",
    "        \"type\" : \"together_ai\",\n",
    "        \"name\" : \"DeepSeek/DeepSeek-R1-70B-Instruct\",\n",
    "        \"api_key\": read_api_key(ROOT_PATH + 'Data/Auth/together.ai.api.key/API.key'),\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e9aba3",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "1. **Load each dataset**\n",
    "2. **forming a uniform dataset**\n",
    "3. **split into train and test sets**\n",
    "4. **Grouping the data instances by their schemas**: We are grouping beased on the schema, because we want to generate the prompts that all questions for similar schemas are grouped together to avoid redundancy and reduce the token count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73e9d35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(bird_path: str = BIRD_DATA_PATH, \n",
    "             spider_path: str = SPIDER_DATA_PATH,\n",
    "             spider2_path: str = SPIDER2_DATA_PATH) -> Tuple[List[Tuple[Dict, str]], List[Tuple[Dict, str]]]:\n",
    "        \"\"\"Load data from BIRD and SPIDER datasets\"\"\"\n",
    "        bird_data = _load_json_files(bird_path)\n",
    "        spider_data = _load_json_files(spider_path)\n",
    "        # spider2\n",
    "        spider2_data = _load_json_files(spider2_path)\n",
    "        \n",
    "        all_data = bird_data + spider_data + spider2_data\n",
    "        \n",
    "        print(f\"Total data points: {len(all_data)}\")\n",
    "        print(f\"Bird data points: {len(bird_data)}\")\n",
    "        print(f\"Spider data points: {len(spider_data)}\")\n",
    "        print(f\"Spider2 data points: {len(spider2_data)}\")\n",
    "\n",
    "        return all_data\n",
    "\n",
    "def _load_json_files(dir_path: str) -> List[Tuple[Dict, str]]:\n",
    "    \"\"\"Load all JSON files from a directory\"\"\"\n",
    "    data = []\n",
    "    for filepath in glob.glob(os.path.join(dir_path, 'instance_*.json')):\n",
    "        with open(filepath, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "            data.append((json_data, filepath))\n",
    "    return data\n",
    "\n",
    "def _group_instances_by_schema(train_data) -> Dict[str, List[Tuple[Dict, str]]]:\n",
    "        \"\"\"Group instances by their database schema\"\"\"\n",
    "        schema_groups = defaultdict(list)\n",
    "        \n",
    "        for instance_data, file_path in train_data:\n",
    "            # Create a unique key for the database schema\n",
    "            db_name = instance_data['database']['name']\n",
    "            dataset_type = instance_data['dataset']\n",
    "            database_type = instance_data['database'].get('type', 'unknown')\n",
    "            schema_key = f\"{dataset_type}_{database_type}_{db_name}\"\n",
    "            \n",
    "            schema_groups[schema_key].append((instance_data, file_path))\n",
    "        \n",
    "        return dict(schema_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50e07ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the snowflake credentials\n",
    "SNOWFLAKE_CREDENTIALS_PATH = ROOT_PATH + 'Data/Spider2/spider2-lite/evaluation_suite/snowflake_credential.json'\n",
    "with open(SNOWFLAKE_CREDENTIALS_PATH, 'r') as file:\n",
    "    snowflake_credentials = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437e1ff6",
   "metadata": {},
   "source": [
    "# Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b39f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 604\n",
      "Bird data points: 250\n",
      "Spider data points: 250\n",
      "Spider2 data points: 104\n",
      "Training data points: 483\n",
      "Testing data points: 121\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>schema_key</th>\n",
       "      <th>instances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bird_sqlite_student_club</td>\n",
       "      <td>[({'id': 1318, 'dataset': 'bird', 'database': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spider_sqlite_cre_Doc_Template_Mgt</td>\n",
       "      <td>[({'id': 369, 'dataset': 'spider', 'database':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spider2-lite_snowflake_IDC</td>\n",
       "      <td>[({'id': 271, 'original_instance_id': 'sf_bq34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bird_sqlite_formula_1</td>\n",
       "      <td>[({'id': 864, 'dataset': 'bird', 'database': {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spider_sqlite_world_1</td>\n",
       "      <td>[({'id': 772, 'dataset': 'spider', 'database':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           schema_key  \\\n",
       "0            bird_sqlite_student_club   \n",
       "1  spider_sqlite_cre_Doc_Template_Mgt   \n",
       "2          spider2-lite_snowflake_IDC   \n",
       "3               bird_sqlite_formula_1   \n",
       "4               spider_sqlite_world_1   \n",
       "\n",
       "                                           instances  \n",
       "0  [({'id': 1318, 'dataset': 'bird', 'database': ...  \n",
       "1  [({'id': 369, 'dataset': 'spider', 'database':...  \n",
       "2  [({'id': 271, 'original_instance_id': 'sf_bq34...  \n",
       "3  [({'id': 864, 'dataset': 'bird', 'database': {...  \n",
       "4  [({'id': 772, 'dataset': 'spider', 'database':...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = load_data()\n",
    "train_data = _group_instances_by_schema(train_data)\n",
    "\n",
    "df = pd.DataFrame(train_data.items(), columns=['schema_key', 'instances'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "710d2e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== The number of instances in each schema group ======\n",
      "The number of total Databases accross the training sets :  75.0\n",
      "The number of average instances per database accross the training sets :  6.44\n"
     ]
    }
   ],
   "source": [
    "print(\"====== The number of instances in each schema group ======\")\n",
    "description = df.apply(lambda x: len(x['instances']), axis=1).describe()\n",
    "print(\"The number of total Databases accross the training sets : \",description['count'])\n",
    "print(\"The number of average instances per database accross the training sets : \",description['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b4604f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a Sample data\n",
    "sample_instance,sample_instance_path = train_data['spider2-lite_sqlite_EntertainmentAgency'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97d83108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/venv/lib/python3.12/site-packages/snowflake/connector/options.py:104: UserWarning: You have an incompatible version of 'pyarrow' installed (19.0.1), please install a version that adheres to: 'pyarrow<19.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n",
      "INFO:pipeline.text2sql_enricher:Initializing OptimizedText2SQLPipeline...\n",
      "INFO:pipeline.text2sql_enricher:Schema understanding logging enabled. Logs will be saved to: schema_understanding_logs/schema_understanding_20250606_230308.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Model is : claude-3-7-sonnet-20250219\n",
      "Sample Schema Introduction Prompt:\n",
      "You are now working with the \"EntertainmentAgency\" database. \n",
      "\n",
      "Here's the complete database schema:\n",
      "\n",
      "## Table: Agents\n",
      "```sql\n",
      "CREATE TABLE Agents (\n",
      "    AgentID INT,\n",
      "    AgtFirstName nvarchar (25),\n",
      "    AgtLastName nvarchar (25),\n",
      "    AgtStreetAddress nvarchar (50),\n",
      "    AgtCity nvarchar (30),\n",
      "    AgtState nvarchar (2),\n",
      "    AgtZipCode nvarchar (10),\n",
      "    AgtPhoneNumber nvarchar (15),\n",
      "    DateHired date,\n",
      "    Salary decimal(15, 2),\n",
      "    CommissionRate float(24)\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: Customers\n",
      "```sql\n",
      "CREATE TABLE Customers (\n",
      "    CustomerID INT,\n",
      "    CustFirstName nvarchar (25),\n",
      "    CustLastName nvarchar (25),\n",
      "    CustStreetAddress nvarchar (50),\n",
      "    CustCity nvarchar (30),\n",
      "    CustState nvarchar (2),\n",
      "    CustZipCode nvarchar (10),\n",
      "    CustPhoneNumber nvarchar (15)\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: Engagements\n",
      "```sql\n",
      "CREATE TABLE Engagements (\n",
      "    EngagementNumber INT,\n",
      "    StartDate date,\n",
      "    EndDate date,\n",
      "    StartTime time,\n",
      "    StopTime time,\n",
      "    ContractPrice decimal(15, 2),\n",
      "    CustomerID INT,\n",
      "    AgentID INT,\n",
      "    EntertainerID INT\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: Entertainer_Members\n",
      "```sql\n",
      "CREATE TABLE Entertainer_Members (\n",
      "    EntertainerID INT,\n",
      "    MemberID INT,\n",
      "    Status smallint\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: Entertainer_Styles\n",
      "```sql\n",
      "CREATE TABLE Entertainer_Styles (\n",
      "    EntertainerID INT,\n",
      "    StyleID smallint,\n",
      "    StyleStrength smallint\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: Entertainers\n",
      "```sql\n",
      "CREATE TABLE Entertainers (\n",
      "    EntertainerID INT,\n",
      "    EntStageName nvarchar (50),\n",
      "    EntSSN nvarchar (12),\n",
      "    EntStreetAddress nvarchar (50),\n",
      "    EntCity nvarchar (30),\n",
      "    EntState nvarchar (2),\n",
      "    EntZipCode nvarchar (10),\n",
      "    EntPhoneNumber nvarchar (15),\n",
      "    EntWebPage nvarchar (50),\n",
      "    EntEMailAddress nvarchar (50),\n",
      "    DateEntered date\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: Members\n",
      "```sql\n",
      "CREATE TABLE Members (\n",
      "    MemberID INT,\n",
      "    MbrFirstName nvarchar (25),\n",
      "    MbrLastName nvarchar (25),\n",
      "    MbrPhoneNumber nvarchar (15),\n",
      "    Gender nvarchar (2)\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: Musical_Preferences\n",
      "```sql\n",
      "CREATE TABLE Musical_Preferences (\n",
      "    CustomerID INT,\n",
      "    StyleID smallint,\n",
      "    PreferenceSeq smallint\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: Musical_Styles\n",
      "```sql\n",
      "CREATE TABLE Musical_Styles (\n",
      "    StyleID smallint,\n",
      "    StyleName nvarchar (75)\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: ztblDays\n",
      "```sql\n",
      "CREATE TABLE ztblDays (\n",
      "    DateField date\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: ztblMonths\n",
      "```sql\n",
      "CREATE TABLE ztblMonths (\n",
      "    MonthYear nvarchar (15),\n",
      "    YearNumber smallint,\n",
      "    MonthNumber smallint,\n",
      "    MonthStart date,\n",
      "    MonthEnd date,\n",
      "    January smallint,\n",
      "    February smallint,\n",
      "    March smallint,\n",
      "    April smallint,\n",
      "    May smallint,\n",
      "    June smallint,\n",
      "    July smallint,\n",
      "    August smallint,\n",
      "    September smallint,\n",
      "    October smallint,\n",
      "    November smallint,\n",
      "    December smallint\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: ztblSkipLabels\n",
      "```sql\n",
      "CREATE TABLE ztblSkipLabels (\n",
      "    LabelCount INT\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: ztblWeeks\n",
      "```sql\n",
      "CREATE TABLE ztblWeeks (\n",
      "    WeekStart date,\n",
      "    WeekEnd date\n",
      ");\n",
      "```\n",
      "\n",
      "Please familiarize yourself with this database structure. I will now ask you a series of questions about this database. \n",
      "\n",
      "For each question:\n",
      "1. Analyze the question carefully\n",
      "2. Consider any additional evidence provided\n",
      "3. Generate a SQL query that answers the question\n",
      "4. Return your response in JSON format: {\"sql\": \"your_sql_query_here\"}\n",
      "\n",
      "Can you understand the database schema fully? If you can, please provide a brief summary of the schema and confirm your understanding.\n"
     ]
    }
   ],
   "source": [
    "from pipeline.text2sql_enricher import OptimizedText2SQLPipeline\n",
    "\n",
    "print(\"This Model is :\", model_configs[0]['name'])\n",
    "\n",
    "pipeline = OptimizedText2SQLPipeline(model_config=model_configs[0],\n",
    "                                     snowflake_config=snowflake_credentials)\n",
    "\n",
    "sample_schema_intro_prompt = pipeline._create_schema_introduction_prompt(sample_instance)\n",
    "\n",
    "print(\"Sample Schema Introduction Prompt:\")\n",
    "print(sample_schema_intro_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "842b40df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.schemahandler import SequentialSchemaHandler\n",
    "\n",
    "pipeline.model_provider.start_new_conversation()\n",
    "\n",
    "schema_handler = SequentialSchemaHandler(pipeline.model_provider,\n",
    "                                         max_tokens_per_chunk=4000,\n",
    "                                         token_threshold=6000,\n",
    "                                         nlp_model='en_core_web_sm')\n",
    "\n",
    "system_message = (\n",
    "                \"You are a database expert specializing in SQL query generation. \"\n",
    "                \"You will be working with a specific database schema and answering \"\n",
    "                \"multiple questions about it. Please pay careful attention to the \"\n",
    "                \"schema structure and relationships between tables.\"\n",
    "            )\n",
    "\n",
    "_,final_response = schema_handler.handle_large_schema(sample_instance, system_message=system_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fad89505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response from Schema Handler:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Response from Schema Handler:\")\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "699f1b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the sample instance:\n",
      "- Agents\n",
      "- Customers\n",
      "- Engagements\n",
      "- Entertainer_Members\n",
      "- Entertainer_Styles\n",
      "- Entertainers\n",
      "- Members\n",
      "- Musical_Preferences\n",
      "- Musical_Styles\n",
      "- ztblDays\n",
      "- ztblMonths\n",
      "- ztblSkipLabels\n",
      "- ztblWeeks\n"
     ]
    }
   ],
   "source": [
    "# get tables names from sample_instance['schemas']\n",
    "table_names = [table['table_name'] for table in sample_instance['schemas']]\n",
    "print(\"Tables in the sample instance:\")\n",
    "for table in table_names:\n",
    "    print(f\"- {table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a81aee6",
   "metadata": {},
   "source": [
    "> The above results comparing between the tables list of model understanding from the whole schema based on `token_windowing` strategy over large database schemas. Where notably, there are some tables that either not included in the schema or not used in the model understanding. This is because the model missed some tables as it the number of tokens per each table schemas is larger than the token limit of the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19b552ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "The question prompt was:\n",
      "Question: Could you list each musical style with the number of times it appears as a 1st, 2nd, or 3rd preference in a single row per style?\n",
      "\n",
      "Please generate the SQL query to answer this question using the database schema we discussed.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SQL Query:\n",
      "SELECT \n",
      "    style_name,\n",
      "    COUNT(CASE WHEN preference_rank = 1 THEN 1 END) AS first_preference,\n",
      "    COUNT(CASE WHEN preference_rank = 2 THEN 1 END) AS second_preference,\n",
      "    COUNT(CASE WHEN preference_rank = 3 THEN 1 END) AS third_preference\n",
      "FROM musical_preferences\n",
      "GROUP BY style_name\n",
      "ORDER BY style_name;\n"
     ]
    }
   ],
   "source": [
    "question_prompt = pipeline._create_question_prompt(sample_instance)\n",
    "\n",
    "print(50 * \"=\")\n",
    "print(\"The question prompt was:\")\n",
    "print(question_prompt)\n",
    "print(50 * \"=\")\n",
    "raw_response = pipeline.model_provider.generate_with_context(\"\", question_prompt)\n",
    "generated_sql = pipeline.extract_sql_query_from_text(raw_response)\n",
    "print(\"Generated SQL Query:\")\n",
    "print(generated_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6210e6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully. Results:\n",
      "{('Rhythm and Blues', 2, 0, 1), ('Folk', 0, 1, 0), ('Country Rock', 1, 0, 0), ('Top 40 Hits', 2, 0, 0), ('Modern Rock', 0, 1, 1), ('Variety', 1, 0, 0), ('Classic Rock & Roll', 1, 1, 0), ('Standards', 2, 2, 0), ('Motown', 0, 1, 0), (\"70's Music\", 0, 1, 0), ('Chamber Music', 1, 0, 0), ('Jazz', 2, 1, 0), ('Show Tunes', 1, 1, 0), ('Contemporary', 1, 2, 0), (\"60's Music\", 1, 0, 0), (\"80's Music\", 0, 0, 1), (\"40's Ballroom Music\", 0, 1, 1), ('Country', 0, 1, 0), ('Salsa', 0, 1, 1), ('Classical', 0, 1, 1)}\n"
     ]
    }
   ],
   "source": [
    "conn,_  = pipeline.get_db_connection(sample_instance,sample_instance_path )\n",
    "\n",
    "cursor = conn.cursor()\n",
    "try:\n",
    "    cursor.execute(sample_instance['sql'])\n",
    "    results = cursor.fetchall()\n",
    "    print(\"Query executed successfully. Results:\")\n",
    "    print(set(results))\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4580d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the past cursor and connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "660fd3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully. Results:\n",
      "{('Yearly Kickoff',)}\n"
     ]
    }
   ],
   "source": [
    "bird_sqlite_sample,path = train_data['bird_sqlite_student_club'][0]\n",
    "\n",
    "conn, _ = pipeline.get_db_connection(bird_sqlite_sample, path)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "try:\n",
    "    cursor.execute(bird_sqlite_sample['sql'])\n",
    "    results = cursor.fetchall()\n",
    "    print(\"Query executed successfully. Results:\")\n",
    "    print(set(results))\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.text2sql_enricher import OptimizedText2SQLPipeline\n",
    "\n",
    "print(\"This Model is :\", model_configs[0]['name'])\n",
    "\n",
    "pipeline = OptimizedText2SQLPipeline(model_config=model_configs[0],\n",
    "                                     snowflake_config=snowflake_credentials)\n",
    "\n",
    "output_dir = ROOT_PATH + 'DataSampling/data/enriched_dataset/enriched_v4'\n",
    "\n",
    "results = pipeline.run_pipeline(\n",
    "    schema_groups=train_data,\n",
    "    save_updated_files=True,\n",
    "    output_dir=output_dir,\n",
    ")\n",
    "\n",
    "# Store summary metrics\n",
    "summary_metrics = {\n",
    "    'num_evaluated': results['num_evaluated'],\n",
    "    'num_with_prediction': results['num_with_prediction'],\n",
    "    'prediction_rate': results['prediction_rate'],\n",
    "    'execution_accuracy': results['execution_accuracy'],\n",
    "    'exact_match_accuracy': results['exact_match_accuracy'],\n",
    "    'semantic_equivalent_accuracy': results.get('semantic_equivalent_accuracy', 0.0),\n",
    "    'model': results['model'],\n",
    "    'optimization': results.get('optimization_used', 'conversational_schema_context')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1e89e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_evaluated': 483,\n",
       " 'num_with_prediction': 373,\n",
       " 'prediction_rate': 0.772256728778468,\n",
       " 'execution_accuracy': 0.46648793565683644,\n",
       " 'exact_match_accuracy': 0.013404825737265416,\n",
       " 'semantic_equivalent_accuracy': 0.49865951742627346,\n",
       " 'model': {'model_name': 'claude-3-7-sonnet-20250219',\n",
       "  'model_type': 'anthropic',\n",
       "  'timestamp': '2025-06-06T23:04:59.084928',\n",
       "  'optimization': 'conversational_schema_context'},\n",
       " 'optimization': 'conversational_schema_context'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f71aacb",
   "metadata": {},
   "source": [
    "# Process the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b493e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = ROOT_PATH + 'DataSampling/data/enriched_dataset/enriched_v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787beeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_instances = output_dir + '/' + 'instance_bird_*.json'\n",
    "spider_instances = output_dir + '/' + 'instance_spider_*.json'\n",
    "spider2_instances = output_dir + '/' + 'instance_spider2-lite_*.json'\n",
    "\n",
    "# ! For example for Claude \n",
    "v2_llama3 = ROOT_PATH + 'DataSampling/data/enriched_dataset/v2_Llama3'\n",
    "\n",
    "bird_set_dir = v2_llama3 + '/bird_set_stratified'\n",
    "spider_set_dir = v2_llama3 + '/spider_set_stratified'\n",
    "spider2_set_dir = v2_llama3 + '/spider2_lite_set'\n",
    "\n",
    "# copy the bird_instances to the v2_llama3\n",
    "#/bird_set_dir\n",
    "import shutil\n",
    "\n",
    "def copy_files(src_pattern: str, dest_dir: str):\n",
    "    for src_file in glob.glob(src_pattern):\n",
    "        shutil.copy(src_file, dest_dir)\n",
    "\n",
    "# copy_files(bird_instances, bird_set_dir)\n",
    "# copy_files(spider_instances, spider_set_dir)\n",
    "# copy_files(spider2_instances, spider2_set_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c3a19d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 604\n",
      "Bird data points: 250\n",
      "Spider data points: 250\n",
      "Spider2 data points: 104\n"
     ]
    }
   ],
   "source": [
    "output_generated_data = load_data(bird_path=bird_set_dir,\n",
    "                                    spider_path=spider_set_dir,\n",
    "                                    spider2_path=spider2_set_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6a96f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total instances with missing SQL queries: 143\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>schema_key</th>\n",
       "      <th>instances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bird_sqlite_superhero</td>\n",
       "      <td>[({'id': 723, 'dataset': 'bird', 'database': {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bird_sqlite_card_games</td>\n",
       "      <td>[({'id': 525, 'dataset': 'bird', 'database': {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bird_sqlite_student_club</td>\n",
       "      <td>[({'id': 1404, 'dataset': 'bird', 'database': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bird_sqlite_european_football_2</td>\n",
       "      <td>[({'id': 1114, 'dataset': 'bird', 'database': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bird_sqlite_toxicology</td>\n",
       "      <td>[({'id': 275, 'dataset': 'bird', 'database': {...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        schema_key  \\\n",
       "0            bird_sqlite_superhero   \n",
       "1           bird_sqlite_card_games   \n",
       "2         bird_sqlite_student_club   \n",
       "3  bird_sqlite_european_football_2   \n",
       "4           bird_sqlite_toxicology   \n",
       "\n",
       "                                           instances  \n",
       "0  [({'id': 723, 'dataset': 'bird', 'database': {...  \n",
       "1  [({'id': 525, 'dataset': 'bird', 'database': {...  \n",
       "2  [({'id': 1404, 'dataset': 'bird', 'database': ...  \n",
       "3  [({'id': 1114, 'dataset': 'bird', 'database': ...  \n",
       "4  [({'id': 275, 'dataset': 'bird', 'database': {...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_sql_data = [item for item in output_generated_data if 'inference_results' not in item[0] or item[0]['inference_results']['has_prediction'] == False]\n",
    "\n",
    "print(f\"Total instances with missing SQL queries: {len(missing_sql_data)}\")\n",
    "\n",
    "missing_grouped = _group_instances_by_schema(missing_sql_data)\n",
    "\n",
    "missing_df = pd.DataFrame(missing_grouped.items(), columns=['schema_key', 'instances'])\n",
    "\n",
    "missing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52e160c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== The number of instances in each schema group ======\n",
      "The number of total Databases accross the training sets :  58.0\n",
      "The number of average instances per database accross the training sets :  2.4655172413793105\n"
     ]
    }
   ],
   "source": [
    "print(\"====== The number of instances in each schema group ======\")\n",
    "description = missing_df.apply(lambda x: len(x['instances']), axis=1).describe()\n",
    "print(\"The number of total Databases accross the training sets : \",description['count'])\n",
    "print(\"The number of average instances per database accross the training sets : \",description['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34613ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_sqlite_Brazilian_E_Commerce: 100%|██████████| 1/1 [00:43<00:00, 43.79s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_sqlite_WWE ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 1\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_sqlite_WWE:   0%|          | 0/1 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 404...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_sqlite_WWE: 100%|██████████| 1/1 [00:05<00:00,  5.60s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_sqlite_Pagila ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 2\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 4.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.756750 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_sqlite_Pagila:   0%|          | 0/2 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 421...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 6.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_sqlite_Pagila:  50%|█████     | 1/2 [00:10<00:10, 10.81s/instance]INFO:pipeline.text2sql_enricher:Processing instance 420...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 6.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_sqlite_Pagila: 100%|██████████| 2/2 [00:21<00:00, 10.76s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_snowflake_GITHUB_REPOS_DATE ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large schema detected. Sending in 18 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 7.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 1/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 2/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 8.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 3/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 4/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 8.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI format conversation error: Error code: 422 - {'id': 'nxCrYyk-3NKUce-94c297edce221575', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7185 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 10.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 5/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI format conversation error: Error code: 422 - {'id': 'nxCreT3-3NKUce-94c2985f193f0e25', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7185 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 7.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 6/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 6.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 10.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI format conversation error: Error code: 422 - {'id': 'nxCrkkr-3NKUce-94c298e87c9aed47', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7185 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.830358 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 7.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 7/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.938963 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI format conversation error: Error code: 422 - {'id': 'nxCrsRG-3NKUce-94c2997709f30d69', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7185 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 9.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 6.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 8/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI format conversation error: Error code: 422 - {'id': 'nxCryqK-4Yz4kd-94c299fa7d1fedd2', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7185 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 9/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 6.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI format conversation error: Error code: 422 - {'id': 'nxCs3AP-3NKUce-94c29a42d8ef7578', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7185 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 10/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI format conversation error: Error code: 422 - {'id': 'nxCs6JR-4Yz4kd-94c29a855fbf7578', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7185 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 11/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 8.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI format conversation error: Error code: 422 - {'id': 'nxCs9Nt-3NKUce-94c29ac81ef28e0a', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7185 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 12/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 5.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 10.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI format conversation error: Error code: 422 - {'id': 'nxCsCTh-3NKUce-94c29b0a5a0be8b8', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7185 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 13/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 10.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI format conversation error: Error code: 422 - {'id': 'nxCsUK3-3NKUce-94c29c563d13edab', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7185 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 14/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI format conversation error: Error code: 422 - {'id': 'nxCsam5-4Yz4kd-94c29cd9cd6983a9', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7185 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 15/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 7.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI format conversation error: Error code: 422 - {'id': 'nxCsdxD-3NKUce-94c29d204dd47a9b', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7185 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 9.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 16/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 7.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.962917 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI format conversation error: Error code: 422 - {'id': 'nxCskUh-4Yz4kd-94c29da6aa5bee9f', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7185 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 17/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI format conversation error: Error code: 422 - {'id': 'nxCsyoc-3NKUce-94c29ebe0ee1ee9f', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7920 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 9.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.827831 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 18/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing spider2-lite_snowflake_GITHUB_REPOS_DATE:   0%|          | 0/1 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 43...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 8.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.14.1, Python Version: 3.12.7, Platform: macOS-15.5-arm64-arm-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_GITHUB_REPOS_DATE: 100%|██████████| 1/1 [00:15<00:00, 15.00s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_sqlite_Baseball ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 1\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 4.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_sqlite_Baseball:   0%|          | 0/1 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 398...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 5.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_sqlite_Baseball: 100%|██████████| 1/1 [00:48<00:00, 48.32s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_sqlite_f1 ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 1\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_sqlite_f1:   0%|          | 0/1 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 523...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_sqlite_f1: 100%|██████████| 1/1 [00:05<00:00,  5.34s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_snowflake_PATENTS ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 4\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_snowflake_PATENTS:   0%|          | 0/4 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 16...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.920839 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.14.1, Python Version: 3.12.7, Platform: macOS-15.5-arm64-arm-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_PATENTS:  25%|██▌       | 1/4 [00:46<02:19, 46.50s/instance]INFO:pipeline.text2sql_enricher:Processing instance 29...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.14.1, Python Version: 3.12.7, Platform: macOS-15.5-arm64-arm-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_PATENTS:  50%|█████     | 2/4 [00:55<00:48, 24.31s/instance]INFO:pipeline.text2sql_enricher:Processing instance 15...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.14.1, Python Version: 3.12.7, Platform: macOS-15.5-arm64-arm-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_PATENTS:  75%|███████▌  | 3/4 [01:02<00:16, 16.52s/instance]INFO:pipeline.text2sql_enricher:Processing instance 18...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.14.1, Python Version: 3.12.7, Platform: macOS-15.5-arm64-arm-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_PATENTS: 100%|██████████| 4/4 [01:51<00:00, 27.81s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_sqlite_California_Traffic_Collision ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 1\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_sqlite_California_Traffic_Collision:   0%|          | 0/1 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 402...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 7.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_sqlite_California_Traffic_Collision: 100%|██████████| 1/1 [00:14<00:00, 14.57s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_sqlite_EU_soccer ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 1\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 3.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_sqlite_EU_soccer:   0%|          | 0/1 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 489...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 4.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.818774 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_sqlite_EU_soccer: 100%|██████████| 1/1 [01:01<00:00, 61.95s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_sqlite_bank_sales_trading ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 3\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_sqlite_bank_sales_trading:   0%|          | 0/3 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 450...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_sqlite_bank_sales_trading:  33%|███▎      | 1/3 [00:31<01:03, 31.78s/instance]INFO:pipeline.text2sql_enricher:Processing instance 452...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_sqlite_bank_sales_trading:  67%|██████▋   | 2/3 [00:39<00:17, 17.90s/instance]INFO:pipeline.text2sql_enricher:Processing instance 513...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_sqlite_bank_sales_trading: 100%|██████████| 3/3 [01:53<00:00, 37.71s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_snowflake_IDC ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large schema detected. Sending in 4 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 9.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing spider2-lite_snowflake_IDC:   0%|          | 0/2 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 270...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.14.1, Python Version: 3.12.7, Platform: macOS-15.5-arm64-arm-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_IDC:  50%|█████     | 1/2 [00:07<00:07,  7.19s/instance]INFO:pipeline.text2sql_enricher:Processing instance 333...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 3.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.14.1, Python Version: 3.12.7, Platform: macOS-15.5-arm64-arm-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_IDC: 100%|██████████| 2/2 [00:14<00:00,  7.49s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_snowflake_CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 1\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 6.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_snowflake_CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE:   0%|          | 0/1 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 545...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.14.1, Python Version: 3.12.7, Platform: macOS-15.5-arm64-arm-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE: 100%|██████████| 1/1 [01:01<00:00, 61.31s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_snowflake_GEO_OPENSTREETMAP_WORLDPOP ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 1\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_snowflake_GEO_OPENSTREETMAP_WORLDPOP:   0%|          | 0/1 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 101...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 4.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Failed to extract SQL from model response\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_GEO_OPENSTREETMAP_WORLDPOP: 100%|██████████| 1/1 [00:32<00:00, 32.31s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_snowflake_TCGA_MITELMAN ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large schema detected. Sending in 9 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 1/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI format conversation error: Error code: 422 - {'id': 'nxCwskK-4Yz4kd-94c2b14e3c580e13', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7905 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "ERROR:pipeline.text2sql_enricher:Error processing schema group spider2-lite_snowflake_TCGA_MITELMAN: Error code: 422 - {'id': 'nxCwspv-3NKUce-94c2b1505d910e13', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7612 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_snowflake_GITHUB_REPOS ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 3\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 9.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_snowflake_GITHUB_REPOS:   0%|          | 0/3 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 57...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 6.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.14.1, Python Version: 3.12.7, Platform: macOS-15.5-arm64-arm-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_GITHUB_REPOS:  33%|███▎      | 1/3 [00:11<00:23, 11.92s/instance]INFO:pipeline.text2sql_enricher:Processing instance 47...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.14.1, Python Version: 3.12.7, Platform: macOS-15.5-arm64-arm-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_GITHUB_REPOS:  67%|██████▋   | 2/3 [00:22<00:11, 11.38s/instance]INFO:pipeline.text2sql_enricher:Processing instance 55...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.14.1, Python Version: 3.12.7, Platform: macOS-15.5-arm64-arm-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_GITHUB_REPOS: 100%|██████████| 3/3 [00:35<00:00, 11.69s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_snowflake_TCGA ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large schema detected. Sending in 8 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI format conversation error: Error code: 422 - {'id': 'nxCx9HH-3NKUce-94c2b291ffaaed16', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7631 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 10.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "ERROR:pipeline.text2sql_enricher:Error processing schema group spider2-lite_snowflake_TCGA: Error code: 422 - {'id': 'nxCxEfu-3NKUce-94c2b300dacf6da0', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7631 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_sqlite_delivery_center ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 1\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 10.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_sqlite_delivery_center:   0%|          | 0/1 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 486...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 7.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_sqlite_delivery_center: 100%|██████████| 1/1 [01:17<00:00, 77.70s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_sqlite_E_commerce ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 2\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_sqlite_E_commerce:   0%|          | 0/2 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 395...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_sqlite_E_commerce:  50%|█████     | 1/2 [00:10<00:10, 10.98s/instance]INFO:pipeline.text2sql_enricher:Processing instance 396...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_sqlite_E_commerce: 100%|██████████| 2/2 [00:16<00:00,  8.36s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_sqlite_IPL ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 2\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 4.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_sqlite_IPL:   0%|          | 0/2 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 408...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_sqlite_IPL:  50%|█████     | 1/2 [00:08<00:08,  8.71s/instance]INFO:pipeline.text2sql_enricher:Processing instance 409...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_sqlite_IPL: 100%|██████████| 2/2 [00:19<00:00,  9.91s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_snowflake_NOAA_DATA_PLUS ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large schema detected. Sending in 14 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 4.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 1/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 7.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 8.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 2/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 3/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 4/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 7.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 5/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 6/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 7/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 8/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 4.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 9/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 10/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 11/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 12/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 4.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.887978 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 13/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 14/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing spider2-lite_snowflake_NOAA_DATA_PLUS:   0%|          | 0/1 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 71...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.14.1, Python Version: 3.12.7, Platform: macOS-15.5-arm64-arm-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_NOAA_DATA_PLUS: 100%|██████████| 1/1 [00:10<00:00, 10.19s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_snowflake_THELOOK_ECOMMERCE ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 1\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_snowflake_THELOOK_ECOMMERCE:   0%|          | 0/1 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 163...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.14.1, Python Version: 3.12.7, Platform: macOS-15.5-arm64-arm-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_THELOOK_ECOMMERCE: 100%|██████████| 1/1 [00:10<00:00, 10.34s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_sqlite_modern_data ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 2\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_sqlite_modern_data:   0%|          | 0/2 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 442...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from pipeline.text2sql_enricher import OptimizedText2SQLPipeline\n",
    "\n",
    "print(\"This Model is :\", model_configs[1]['name'])\n",
    "\n",
    "pipeline = OptimizedText2SQLPipeline(model_config=model_configs[1],\n",
    "                                     snowflake_config=snowflake_credentials)\n",
    "\n",
    "results = pipeline.run_pipeline(\n",
    "    schema_groups=missing_grouped,\n",
    "    save_updated_files=True\n",
    ")\n",
    "\n",
    "# Store summary metrics\n",
    "summary_metrics = {\n",
    "    'num_evaluated': results['num_evaluated'],\n",
    "    'num_with_prediction': results['num_with_prediction'],\n",
    "    'prediction_rate': results['prediction_rate'],\n",
    "    'execution_accuracy': results['execution_accuracy'],\n",
    "    'exact_match_accuracy': results['exact_match_accuracy'],\n",
    "    'semantic_equivalent_accuracy': results.get('semantic_equivalent_accuracy', 0.0),\n",
    "    'model': results['model'],\n",
    "    'optimization': results.get('optimization_used', 'conversational_schema_context')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "400ba70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_evaluated': 230,\n",
       " 'num_with_prediction': 230,\n",
       " 'prediction_rate': 1.0,\n",
       " 'execution_accuracy': 0.36086956521739133,\n",
       " 'exact_match_accuracy': 0.05217391304347826,\n",
       " 'semantic_equivalent_accuracy': 0.3826086956521739,\n",
       " 'model': {'model_name': 'claude-3-7-sonnet-20250219',\n",
       "  'model_type': 'anthropic',\n",
       "  'timestamp': '2025-06-07T18:24:56.724602',\n",
       "  'optimization': 'conversational_schema_context'},\n",
       " 'optimization': 'conversational_schema_context'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced10f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
