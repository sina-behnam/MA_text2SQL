{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf965f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Any, Optional, Union, Callable\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sqlparse\n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT_PATH = '/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/'\n",
    "\n",
    "import sys\n",
    "sys.path.append(ROOT_PATH + 'DataSampling/src/models/pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03931798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_api_key(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads the API key from a file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"API key file not found: {file_path}\")\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        api_key = file.read().strip()\n",
    "    \n",
    "    if not api_key:\n",
    "        raise ValueError(\"API key is empty.\")\n",
    "    \n",
    "    return api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae1e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIRD_DATA_PATH = ROOT_PATH + 'DataSampling/data/enriched_dataset/v2/bird_set_stratified'\n",
    "SPIDER_DATA_PATH = ROOT_PATH + 'DataSampling/data/enriched_dataset/v2/spider_set_stratified'\n",
    "SPIDER2_DATA_PATH = ROOT_PATH + 'DataSampling/data/enriched_dataset/v2/spider2_lite_set'\n",
    "\n",
    "model_configs = [\n",
    "    # Anthropic Claude with extended thinking\n",
    "    {\n",
    "        \"type\": \"anthropic\",\n",
    "        \"name\": \"claude-3-7-sonnet-20250219\",\n",
    "        \"api_key\": read_api_key(ROOT_PATH + 'Data/Auth/anthropic.api.key/text2sql.key'),\n",
    "        \"extended_thinking\": True,\n",
    "    },\n",
    "    # Together.ai model\n",
    "    {\n",
    "        \"type\": \"together_ai\",\n",
    "        \"name\": \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "        \"api_key\": read_api_key(ROOT_PATH + 'Data/Auth/together.ai.api.key/API.key'),\n",
    "    },\n",
    "    # Together.ai model DeepSeek R1 \n",
    "    {\n",
    "        \"type\" : \"together_ai\",\n",
    "        \"name\" : \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free\",\n",
    "        \"api_key\": read_api_key(ROOT_PATH + 'Data/Auth/together.ai.api.key/API.key'),\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e9aba3",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "1. **Load each dataset**\n",
    "2. **forming a uniform dataset**\n",
    "3. **split into train and test sets**\n",
    "4. **Grouping the data instances by their schemas**: We are grouping beased on the schema, because we want to generate the prompts that all questions for similar schemas are grouped together to avoid redundancy and reduce the token count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73e9d35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(bird_path: str = BIRD_DATA_PATH, \n",
    "             spider_path: str = SPIDER_DATA_PATH,\n",
    "             spider2_path: str = SPIDER2_DATA_PATH) -> Tuple[List[Tuple[Dict, str]], List[Tuple[Dict, str]]]:\n",
    "        \"\"\"Load data from BIRD and SPIDER datasets\"\"\"\n",
    "        bird_data = _load_json_files(bird_path)\n",
    "        spider_data = _load_json_files(spider_path)\n",
    "        # spider2\n",
    "        spider2_data = _load_json_files(spider2_path)\n",
    "        \n",
    "        all_data = bird_data + spider_data + spider2_data\n",
    "        \n",
    "        print(f\"Total data points: {len(all_data)}\")\n",
    "        print(f\"Bird data points: {len(bird_data)}\")\n",
    "        print(f\"Spider data points: {len(spider_data)}\")\n",
    "        print(f\"Spider2 data points: {len(spider2_data)}\")\n",
    "\n",
    "        return all_data\n",
    "\n",
    "def _load_json_files(dir_path: str) -> List[Tuple[Dict, str]]:\n",
    "    \"\"\"Load all JSON files from a directory\"\"\"\n",
    "    data = []\n",
    "    for filepath in glob.glob(os.path.join(dir_path, 'instance_*.json')):\n",
    "        with open(filepath, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "            data.append((json_data, filepath))\n",
    "    return data\n",
    "\n",
    "def _group_instances_by_schema(train_data) -> Dict[str, List[Tuple[Dict, str]]]:\n",
    "        \"\"\"Group instances by their database schema\"\"\"\n",
    "        schema_groups = defaultdict(list)\n",
    "        \n",
    "        for instance_data, file_path in train_data:\n",
    "            # Create a unique key for the database schema\n",
    "            db_name = instance_data['database']['name']\n",
    "            dataset_type = instance_data['dataset']\n",
    "            database_type = instance_data['database'].get('type', 'unknown')\n",
    "            schema_key = f\"{dataset_type}_{database_type}_{db_name}\"\n",
    "            \n",
    "            schema_groups[schema_key].append((instance_data, file_path))\n",
    "        \n",
    "        return dict(schema_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50e07ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the snowflake credentials\n",
    "SNOWFLAKE_CREDENTIALS_PATH = ROOT_PATH + 'Data/Spider2/spider2-lite/evaluation_suite/snowflake_credential.json'\n",
    "with open(SNOWFLAKE_CREDENTIALS_PATH, 'r') as file:\n",
    "    snowflake_credentials = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437e1ff6",
   "metadata": {},
   "source": [
    "# Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b39f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 604\n",
      "Bird data points: 250\n",
      "Spider data points: 250\n",
      "Spider2 data points: 104\n",
      "Training data points: 483\n",
      "Testing data points: 121\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>schema_key</th>\n",
       "      <th>instances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bird_sqlite_student_club</td>\n",
       "      <td>[({'id': 1318, 'dataset': 'bird', 'database': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spider_sqlite_cre_Doc_Template_Mgt</td>\n",
       "      <td>[({'id': 369, 'dataset': 'spider', 'database':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spider2-lite_snowflake_IDC</td>\n",
       "      <td>[({'id': 271, 'original_instance_id': 'sf_bq34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bird_sqlite_formula_1</td>\n",
       "      <td>[({'id': 864, 'dataset': 'bird', 'database': {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spider_sqlite_world_1</td>\n",
       "      <td>[({'id': 772, 'dataset': 'spider', 'database':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           schema_key  \\\n",
       "0            bird_sqlite_student_club   \n",
       "1  spider_sqlite_cre_Doc_Template_Mgt   \n",
       "2          spider2-lite_snowflake_IDC   \n",
       "3               bird_sqlite_formula_1   \n",
       "4               spider_sqlite_world_1   \n",
       "\n",
       "                                           instances  \n",
       "0  [({'id': 1318, 'dataset': 'bird', 'database': ...  \n",
       "1  [({'id': 369, 'dataset': 'spider', 'database':...  \n",
       "2  [({'id': 271, 'original_instance_id': 'sf_bq34...  \n",
       "3  [({'id': 864, 'dataset': 'bird', 'database': {...  \n",
       "4  [({'id': 772, 'dataset': 'spider', 'database':...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = load_data()\n",
    "train_data = _group_instances_by_schema(train_data)\n",
    "\n",
    "df = pd.DataFrame(train_data.items(), columns=['schema_key', 'instances'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "710d2e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== The number of instances in each schema group ======\n",
      "The number of total Databases accross the training sets :  75.0\n",
      "The number of average instances per database accross the training sets :  6.44\n"
     ]
    }
   ],
   "source": [
    "print(\"====== The number of instances in each schema group ======\")\n",
    "description = df.apply(lambda x: len(x['instances']), axis=1).describe()\n",
    "print(\"The number of total Databases accross the training sets : \",description['count'])\n",
    "print(\"The number of average instances per database accross the training sets : \",description['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b4604f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a Sample data\n",
    "sample_instance,sample_instance_path = train_data['spider2-lite_sqlite_EntertainmentAgency'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97d83108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/venv/lib/python3.12/site-packages/snowflake/connector/options.py:104: UserWarning: You have an incompatible version of 'pyarrow' installed (19.0.1), please install a version that adheres to: 'pyarrow<19.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n",
      "INFO:pipeline.text2sql_enricher:Initializing OptimizedText2SQLPipeline...\n",
      "INFO:pipeline.text2sql_enricher:Schema understanding logging enabled. Logs will be saved to: schema_understanding_logs/schema_understanding_20250606_230308.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Model is : claude-3-7-sonnet-20250219\n",
      "Sample Schema Introduction Prompt:\n",
      "You are now working with the \"EntertainmentAgency\" database. \n",
      "\n",
      "Here's the complete database schema:\n",
      "\n",
      "## Table: Agents\n",
      "```sql\n",
      "CREATE TABLE Agents (\n",
      "    AgentID INT,\n",
      "    AgtFirstName nvarchar (25),\n",
      "    AgtLastName nvarchar (25),\n",
      "    AgtStreetAddress nvarchar (50),\n",
      "    AgtCity nvarchar (30),\n",
      "    AgtState nvarchar (2),\n",
      "    AgtZipCode nvarchar (10),\n",
      "    AgtPhoneNumber nvarchar (15),\n",
      "    DateHired date,\n",
      "    Salary decimal(15, 2),\n",
      "    CommissionRate float(24)\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: Customers\n",
      "```sql\n",
      "CREATE TABLE Customers (\n",
      "    CustomerID INT,\n",
      "    CustFirstName nvarchar (25),\n",
      "    CustLastName nvarchar (25),\n",
      "    CustStreetAddress nvarchar (50),\n",
      "    CustCity nvarchar (30),\n",
      "    CustState nvarchar (2),\n",
      "    CustZipCode nvarchar (10),\n",
      "    CustPhoneNumber nvarchar (15)\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: Engagements\n",
      "```sql\n",
      "CREATE TABLE Engagements (\n",
      "    EngagementNumber INT,\n",
      "    StartDate date,\n",
      "    EndDate date,\n",
      "    StartTime time,\n",
      "    StopTime time,\n",
      "    ContractPrice decimal(15, 2),\n",
      "    CustomerID INT,\n",
      "    AgentID INT,\n",
      "    EntertainerID INT\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: Entertainer_Members\n",
      "```sql\n",
      "CREATE TABLE Entertainer_Members (\n",
      "    EntertainerID INT,\n",
      "    MemberID INT,\n",
      "    Status smallint\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: Entertainer_Styles\n",
      "```sql\n",
      "CREATE TABLE Entertainer_Styles (\n",
      "    EntertainerID INT,\n",
      "    StyleID smallint,\n",
      "    StyleStrength smallint\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: Entertainers\n",
      "```sql\n",
      "CREATE TABLE Entertainers (\n",
      "    EntertainerID INT,\n",
      "    EntStageName nvarchar (50),\n",
      "    EntSSN nvarchar (12),\n",
      "    EntStreetAddress nvarchar (50),\n",
      "    EntCity nvarchar (30),\n",
      "    EntState nvarchar (2),\n",
      "    EntZipCode nvarchar (10),\n",
      "    EntPhoneNumber nvarchar (15),\n",
      "    EntWebPage nvarchar (50),\n",
      "    EntEMailAddress nvarchar (50),\n",
      "    DateEntered date\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: Members\n",
      "```sql\n",
      "CREATE TABLE Members (\n",
      "    MemberID INT,\n",
      "    MbrFirstName nvarchar (25),\n",
      "    MbrLastName nvarchar (25),\n",
      "    MbrPhoneNumber nvarchar (15),\n",
      "    Gender nvarchar (2)\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: Musical_Preferences\n",
      "```sql\n",
      "CREATE TABLE Musical_Preferences (\n",
      "    CustomerID INT,\n",
      "    StyleID smallint,\n",
      "    PreferenceSeq smallint\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: Musical_Styles\n",
      "```sql\n",
      "CREATE TABLE Musical_Styles (\n",
      "    StyleID smallint,\n",
      "    StyleName nvarchar (75)\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: ztblDays\n",
      "```sql\n",
      "CREATE TABLE ztblDays (\n",
      "    DateField date\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: ztblMonths\n",
      "```sql\n",
      "CREATE TABLE ztblMonths (\n",
      "    MonthYear nvarchar (15),\n",
      "    YearNumber smallint,\n",
      "    MonthNumber smallint,\n",
      "    MonthStart date,\n",
      "    MonthEnd date,\n",
      "    January smallint,\n",
      "    February smallint,\n",
      "    March smallint,\n",
      "    April smallint,\n",
      "    May smallint,\n",
      "    June smallint,\n",
      "    July smallint,\n",
      "    August smallint,\n",
      "    September smallint,\n",
      "    October smallint,\n",
      "    November smallint,\n",
      "    December smallint\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: ztblSkipLabels\n",
      "```sql\n",
      "CREATE TABLE ztblSkipLabels (\n",
      "    LabelCount INT\n",
      ");\n",
      "```\n",
      "\n",
      "## Table: ztblWeeks\n",
      "```sql\n",
      "CREATE TABLE ztblWeeks (\n",
      "    WeekStart date,\n",
      "    WeekEnd date\n",
      ");\n",
      "```\n",
      "\n",
      "Please familiarize yourself with this database structure. I will now ask you a series of questions about this database. \n",
      "\n",
      "For each question:\n",
      "1. Analyze the question carefully\n",
      "2. Consider any additional evidence provided\n",
      "3. Generate a SQL query that answers the question\n",
      "4. Return your response in JSON format: {\"sql\": \"your_sql_query_here\"}\n",
      "\n",
      "Can you understand the database schema fully? If you can, please provide a brief summary of the schema and confirm your understanding.\n"
     ]
    }
   ],
   "source": [
    "from pipeline.text2sql_enricher import OptimizedText2SQLPipeline\n",
    "\n",
    "print(\"This Model is :\", model_configs[0]['name'])\n",
    "\n",
    "pipeline = OptimizedText2SQLPipeline(model_config=model_configs[0],\n",
    "                                     snowflake_config=snowflake_credentials)\n",
    "\n",
    "sample_schema_intro_prompt = pipeline._create_schema_introduction_prompt(sample_instance)\n",
    "\n",
    "print(\"Sample Schema Introduction Prompt:\")\n",
    "print(sample_schema_intro_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "842b40df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.schemahandler import SequentialSchemaHandler\n",
    "\n",
    "pipeline.model_provider.start_new_conversation()\n",
    "\n",
    "schema_handler = SequentialSchemaHandler(pipeline.model_provider,\n",
    "                                         max_tokens_per_chunk=4000,\n",
    "                                         token_threshold=6000,\n",
    "                                         nlp_model='en_core_web_sm')\n",
    "\n",
    "system_message = (\n",
    "                \"You are a database expert specializing in SQL query generation. \"\n",
    "                \"You will be working with a specific database schema and answering \"\n",
    "                \"multiple questions about it. Please pay careful attention to the \"\n",
    "                \"schema structure and relationships between tables.\"\n",
    "            )\n",
    "\n",
    "_,final_response = schema_handler.handle_large_schema(sample_instance, system_message=system_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fad89505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response from Schema Handler:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Response from Schema Handler:\")\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "699f1b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the sample instance:\n",
      "- Agents\n",
      "- Customers\n",
      "- Engagements\n",
      "- Entertainer_Members\n",
      "- Entertainer_Styles\n",
      "- Entertainers\n",
      "- Members\n",
      "- Musical_Preferences\n",
      "- Musical_Styles\n",
      "- ztblDays\n",
      "- ztblMonths\n",
      "- ztblSkipLabels\n",
      "- ztblWeeks\n"
     ]
    }
   ],
   "source": [
    "# get tables names from sample_instance['schemas']\n",
    "table_names = [table['table_name'] for table in sample_instance['schemas']]\n",
    "print(\"Tables in the sample instance:\")\n",
    "for table in table_names:\n",
    "    print(f\"- {table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a81aee6",
   "metadata": {},
   "source": [
    "> The above results comparing between the tables list of model understanding from the whole schema based on `token_windowing` strategy over large database schemas. Where notably, there are some tables that either not included in the schema or not used in the model understanding. This is because the model missed some tables as it the number of tokens per each table schemas is larger than the token limit of the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19b552ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "The question prompt was:\n",
      "Question: Could you list each musical style with the number of times it appears as a 1st, 2nd, or 3rd preference in a single row per style?\n",
      "\n",
      "Please generate the SQL query to answer this question using the database schema we discussed.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SQL Query:\n",
      "SELECT \n",
      "    style_name,\n",
      "    COUNT(CASE WHEN preference_rank = 1 THEN 1 END) AS first_preference,\n",
      "    COUNT(CASE WHEN preference_rank = 2 THEN 1 END) AS second_preference,\n",
      "    COUNT(CASE WHEN preference_rank = 3 THEN 1 END) AS third_preference\n",
      "FROM musical_preferences\n",
      "GROUP BY style_name\n",
      "ORDER BY style_name;\n"
     ]
    }
   ],
   "source": [
    "question_prompt = pipeline._create_question_prompt(sample_instance)\n",
    "\n",
    "print(50 * \"=\")\n",
    "print(\"The question prompt was:\")\n",
    "print(question_prompt)\n",
    "print(50 * \"=\")\n",
    "raw_response = pipeline.model_provider.generate_with_context(\"\", question_prompt)\n",
    "generated_sql = pipeline.extract_sql_query_from_text(raw_response)\n",
    "print(\"Generated SQL Query:\")\n",
    "print(generated_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6210e6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully. Results:\n",
      "{('Rhythm and Blues', 2, 0, 1), ('Folk', 0, 1, 0), ('Country Rock', 1, 0, 0), ('Top 40 Hits', 2, 0, 0), ('Modern Rock', 0, 1, 1), ('Variety', 1, 0, 0), ('Classic Rock & Roll', 1, 1, 0), ('Standards', 2, 2, 0), ('Motown', 0, 1, 0), (\"70's Music\", 0, 1, 0), ('Chamber Music', 1, 0, 0), ('Jazz', 2, 1, 0), ('Show Tunes', 1, 1, 0), ('Contemporary', 1, 2, 0), (\"60's Music\", 1, 0, 0), (\"80's Music\", 0, 0, 1), (\"40's Ballroom Music\", 0, 1, 1), ('Country', 0, 1, 0), ('Salsa', 0, 1, 1), ('Classical', 0, 1, 1)}\n"
     ]
    }
   ],
   "source": [
    "conn,_  = pipeline.get_db_connection(sample_instance,sample_instance_path )\n",
    "\n",
    "cursor = conn.cursor()\n",
    "try:\n",
    "    cursor.execute(sample_instance['sql'])\n",
    "    results = cursor.fetchall()\n",
    "    print(\"Query executed successfully. Results:\")\n",
    "    print(set(results))\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4580d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the past cursor and connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "660fd3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully. Results:\n",
      "{('Yearly Kickoff',)}\n"
     ]
    }
   ],
   "source": [
    "bird_sqlite_sample,path = train_data['bird_sqlite_student_club'][0]\n",
    "\n",
    "conn, _ = pipeline.get_db_connection(bird_sqlite_sample, path)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "try:\n",
    "    cursor.execute(bird_sqlite_sample['sql'])\n",
    "    results = cursor.fetchall()\n",
    "    print(\"Query executed successfully. Results:\")\n",
    "    print(set(results))\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.text2sql_enricher import OptimizedText2SQLPipeline\n",
    "\n",
    "print(\"This Model is :\", model_configs[0]['name'])\n",
    "\n",
    "pipeline = OptimizedText2SQLPipeline(model_config=model_configs[0],\n",
    "                                     snowflake_config=snowflake_credentials)\n",
    "\n",
    "output_dir = ROOT_PATH + 'DataSampling/data/enriched_dataset/enriched_v4'\n",
    "\n",
    "results = pipeline.run_pipeline(\n",
    "    schema_groups=train_data,\n",
    "    save_updated_files=True,\n",
    "    output_dir=output_dir,\n",
    ")\n",
    "\n",
    "# Store summary metrics\n",
    "summary_metrics = {\n",
    "    'num_evaluated': results['num_evaluated'],\n",
    "    'num_with_prediction': results['num_with_prediction'],\n",
    "    'prediction_rate': results['prediction_rate'],\n",
    "    'execution_accuracy': results['execution_accuracy'],\n",
    "    'exact_match_accuracy': results['exact_match_accuracy'],\n",
    "    'semantic_equivalent_accuracy': results.get('semantic_equivalent_accuracy', 0.0),\n",
    "    'model': results['model'],\n",
    "    'optimization': results.get('optimization_used', 'conversational_schema_context')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1e89e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_evaluated': 483,\n",
       " 'num_with_prediction': 373,\n",
       " 'prediction_rate': 0.772256728778468,\n",
       " 'execution_accuracy': 0.46648793565683644,\n",
       " 'exact_match_accuracy': 0.013404825737265416,\n",
       " 'semantic_equivalent_accuracy': 0.49865951742627346,\n",
       " 'model': {'model_name': 'claude-3-7-sonnet-20250219',\n",
       "  'model_type': 'anthropic',\n",
       "  'timestamp': '2025-06-06T23:04:59.084928',\n",
       "  'optimization': 'conversational_schema_context'},\n",
       " 'optimization': 'conversational_schema_context'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f71aacb",
   "metadata": {},
   "source": [
    "# Process the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b493e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = ROOT_PATH + 'DataSampling/data/enriched_dataset/enriched_v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787beeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_instances = output_dir + '/' + 'instance_bird_*.json'\n",
    "spider_instances = output_dir + '/' + 'instance_spider_*.json'\n",
    "spider2_instances = output_dir + '/' + 'instance_spider2-lite_*.json'\n",
    "\n",
    "# ! \n",
    "v2_llama3 = ROOT_PATH + 'DataSampling/data/enriched_dataset/v2_Llama3'\n",
    "\n",
    "bird_set_dir = v2_llama3 + '/bird_set_stratified'\n",
    "spider_set_dir = v2_llama3 + '/spider_set_stratified'\n",
    "spider2_set_dir = v2_llama3 + '/spider2_lite_set'\n",
    "\n",
    "# copy the bird_instances to the v2_llama3\n",
    "#/bird_set_dir\n",
    "import shutil\n",
    "\n",
    "def copy_files(src_pattern: str, dest_dir: str):\n",
    "    for src_file in glob.glob(src_pattern):\n",
    "        shutil.copy(src_file, dest_dir)\n",
    "\n",
    "# copy_files(bird_instances, bird_set_dir)\n",
    "# copy_files(spider_instances, spider_set_dir)\n",
    "# copy_files(spider2_instances, spider2_set_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c3a19d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 604\n",
      "Bird data points: 250\n",
      "Spider data points: 250\n",
      "Spider2 data points: 104\n"
     ]
    }
   ],
   "source": [
    "v2_llama3 = ROOT_PATH + 'DataSampling/data/enriched_dataset/v2_Llama3'\n",
    "\n",
    "bird_set_dir = v2_llama3 + '/bird_set_stratified'\n",
    "spider_set_dir = v2_llama3 + '/spider_set_stratified'\n",
    "spider2_set_dir = v2_llama3 + '/spider2_lite_set'\n",
    "\n",
    "output_generated_data = load_data(bird_path=bird_set_dir,\n",
    "                                    spider_path=spider_set_dir,\n",
    "                                    spider2_path=spider2_set_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6a96f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total instances with missing SQL queries: 17\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>schema_key</th>\n",
       "      <th>instances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spider2-lite_snowflake_GEO_OPENSTREETMAP_WORLDPOP</td>\n",
       "      <td>[({'id': 101, 'original_instance_id': 'sf_bq25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spider2-lite_snowflake_TCGA_MITELMAN</td>\n",
       "      <td>[({'id': 306, 'original_instance_id': 'sf_bq16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spider2-lite_snowflake_TCGA</td>\n",
       "      <td>[({'id': 283, 'original_instance_id': 'sf_bq04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spider2-lite_sqlite_modern_data</td>\n",
       "      <td>[({'id': 442, 'original_instance_id': 'local06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spider2-lite_snowflake_HUMAN_GENOME_VARIANTS</td>\n",
       "      <td>[({'id': 122, 'original_instance_id': 'sf_bq03...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          schema_key  \\\n",
       "0  spider2-lite_snowflake_GEO_OPENSTREETMAP_WORLDPOP   \n",
       "1               spider2-lite_snowflake_TCGA_MITELMAN   \n",
       "2                        spider2-lite_snowflake_TCGA   \n",
       "3                    spider2-lite_sqlite_modern_data   \n",
       "4       spider2-lite_snowflake_HUMAN_GENOME_VARIANTS   \n",
       "\n",
       "                                           instances  \n",
       "0  [({'id': 101, 'original_instance_id': 'sf_bq25...  \n",
       "1  [({'id': 306, 'original_instance_id': 'sf_bq16...  \n",
       "2  [({'id': 283, 'original_instance_id': 'sf_bq04...  \n",
       "3  [({'id': 442, 'original_instance_id': 'local06...  \n",
       "4  [({'id': 122, 'original_instance_id': 'sf_bq03...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_sql_data = [item for item in output_generated_data if 'inference_results' not in item[0] or item[0]['inference_results']['has_prediction'] == False]\n",
    "\n",
    "print(f\"Total instances with missing SQL queries: {len(missing_sql_data)}\")\n",
    "\n",
    "missing_grouped = _group_instances_by_schema(missing_sql_data)\n",
    "\n",
    "missing_df = pd.DataFrame(missing_grouped.items(), columns=['schema_key', 'instances'])\n",
    "\n",
    "missing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52e160c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== The number of instances in each schema group ======\n",
      "The number of total Databases accross the training sets :  13.0\n",
      "The number of average instances per database accross the training sets :  1.3076923076923077\n"
     ]
    }
   ],
   "source": [
    "print(\"====== The number of instances in each schema group ======\")\n",
    "description = missing_df.apply(lambda x: len(x['instances']), axis=1).describe()\n",
    "print(\"The number of total Databases accross the training sets : \",description['count'])\n",
    "print(\"The number of average instances per database accross the training sets : \",description['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34613ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/venv/lib/python3.12/site-packages/snowflake/connector/options.py:104: UserWarning: You have an incompatible version of 'pyarrow' installed (19.0.1), please install a version that adheres to: 'pyarrow<19.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n",
      "/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:pipeline.text2sql_enricher:Initializing OptimizedText2SQLPipeline...\n",
      "INFO:pipeline.text2sql_enricher:Schema understanding logging enabled. Logs will be saved to: schema_understanding_logs/schema_understanding_20250608_004322.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Model is : meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_snowflake_GEO_OPENSTREETMAP_WORLDPOP ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 1\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_snowflake_GEO_OPENSTREETMAP_WORLDPOP:   0%|          | 0/1 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 101...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Failed to extract SQL from model response\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_GEO_OPENSTREETMAP_WORLDPOP: 100%|██████████| 1/1 [00:07<00:00,  7.06s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_snowflake_TCGA_MITELMAN ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large schema detected. Sending in 9 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent schema chunk 1/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI format conversation error: Error code: 422 - {'id': 'nxDrRtB-4Yz4kd-94c3aba5af47ee6d', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7878 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 6.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "ERROR:pipeline.text2sql_enricher:Error processing schema group spider2-lite_snowflake_TCGA_MITELMAN: Error code: 422 - {'id': 'nxDrTu5-4Yz4kd-94c3abcfa878ed94', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7612 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_snowflake_TCGA ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large schema detected. Sending in 8 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 3.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.828335 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI format conversation error: Error code: 422 - {'id': 'nxDrVwF-3NKUce-94c3abfbbe48ed94', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7633 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "ERROR:pipeline.text2sql_enricher:Error processing schema group spider2-lite_snowflake_TCGA: Error code: 422 - {'id': 'nxDrW6q-4Yz4kd-94c3abfe5960ed94', 'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7631 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_sqlite_modern_data ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 2\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 9.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_sqlite_modern_data:   0%|          | 0/2 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 442...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 3.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_sqlite_modern_data:  50%|█████     | 1/2 [00:08<00:08,  8.67s/instance]INFO:pipeline.text2sql_enricher:Processing instance 443...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_sqlite_modern_data: 100%|██████████| 2/2 [00:17<00:00,  8.90s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_snowflake_HUMAN_GENOME_VARIANTS ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 1\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 4.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_snowflake_HUMAN_GENOME_VARIANTS:   0%|          | 0/1 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 122...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.14.1, Python Version: 3.12.7, Platform: macOS-15.5-arm64-arm-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_HUMAN_GENOME_VARIANTS: 100%|██████████| 1/1 [00:11<00:00, 11.89s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_snowflake_CENSUS_BUREAU_ACS_2 ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 1\n",
      "ERROR:pipeline.text2sql_enricher:Error processing schema group spider2-lite_snowflake_CENSUS_BUREAU_ACS_2: [E088] Text of length 2238960 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_snowflake_DEATH ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 1\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_snowflake_DEATH:   0%|          | 0/1 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 368...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.14.1, Python Version: 3.12.7, Platform: macOS-15.5-arm64-arm-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_DEATH: 100%|██████████| 1/1 [00:06<00:00,  6.04s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_sqlite_EntertainmentAgency ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 1\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_sqlite_EntertainmentAgency:   0%|          | 0/1 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 463...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: True\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: True\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_sqlite_EntertainmentAgency: 100%|██████████| 1/1 [00:05<00:00,  5.08s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_sqlite_education_business ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 2\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_sqlite_education_business:   0%|          | 0/2 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 429...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_sqlite_education_business:  50%|█████     | 1/2 [00:07<00:07,  7.02s/instance]INFO:pipeline.text2sql_enricher:Processing instance 471...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_sqlite_education_business: 100%|██████████| 2/2 [00:13<00:00,  6.99s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_snowflake_PATENTSVIEW ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 1\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_snowflake_PATENTSVIEW:   0%|          | 0/1 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 35...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.14.1, Python Version: 3.12.7, Platform: macOS-15.5-arm64-arm-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_PATENTSVIEW: 100%|██████████| 1/1 [00:18<00:00, 18.70s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_snowflake_GOOGLE_ADS ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 1\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_snowflake_GOOGLE_ADS:   0%|          | 0/1 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 330...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.14.1, Python Version: 3.12.7, Platform: macOS-15.5-arm64-arm-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_GOOGLE_ADS: 100%|██████████| 1/1 [00:09<00:00,  9.64s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_snowflake_CRYPTO ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 2\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_snowflake_CRYPTO:   0%|          | 0/2 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 117...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.760993 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.14.1, Python Version: 3.12.7, Platform: macOS-15.5-arm64-arm-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_CRYPTO:  50%|█████     | 1/2 [00:23<00:23, 23.06s/instance]INFO:pipeline.text2sql_enricher:Processing instance 102...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.14.1, Python Version: 3.12.7, Platform: macOS-15.5-arm64-arm-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_CRYPTO: 100%|██████████| 2/2 [00:33<00:00, 16.76s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Processing schema group: spider2-lite_snowflake_ETHEREUM_BLOCKCHAIN ===\n",
      "INFO:pipeline.text2sql_enricher:Number of instances: 1\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:pipeline.text2sql_enricher:Schema introduction completed successfully\n",
      "Processing spider2-lite_snowflake_ETHEREUM_BLOCKCHAIN:   0%|          | 0/1 [00:00<?, ?instance/s]INFO:pipeline.text2sql_enricher:Processing instance 123...\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.14.1, Python Version: 3.12.7, Platform: macOS-15.5-arm64-arm-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:pipeline.text2sql_enricher:Execution correct: False\n",
      "INFO:pipeline.text2sql_enricher:Exact match: False\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalent: False\n",
      "INFO:pipeline.text2sql_enricher:--------------------------------------------------\n",
      "Processing spider2-lite_snowflake_ETHEREUM_BLOCKCHAIN: 100%|██████████| 1/1 [00:09<00:00,  9.02s/instance]\n",
      "INFO:pipeline.text2sql_enricher:\n",
      "=== Final Results ===\n",
      "INFO:pipeline.text2sql_enricher:Prediction rate: 0.92\n",
      "INFO:pipeline.text2sql_enricher:Execution accuracy: 0.08\n",
      "INFO:pipeline.text2sql_enricher:Exact match accuracy: 0.00\n",
      "INFO:pipeline.text2sql_enricher:Semantic equivalence accuracy: 0.08\n",
      "INFO:pipeline.text2sql_enricher:Schema understanding logs saved to: schema_understanding_logs/schema_understanding_20250608_004322.log\n",
      "INFO:pipeline.text2sql_enricher:Structured schema logs saved to: schema_understanding_logs/schema_understanding_20250608_004322.json\n"
     ]
    }
   ],
   "source": [
    "from pipeline.text2sql_enricher import OptimizedText2SQLPipeline\n",
    "\n",
    "print(\"This Model is :\", model_configs[1]['name'])\n",
    "\n",
    "pipeline = OptimizedText2SQLPipeline(model_config=model_configs[1],\n",
    "                                     snowflake_config=snowflake_credentials)\n",
    "\n",
    "results = pipeline.run_pipeline(\n",
    "    schema_groups=missing_grouped,\n",
    "    save_updated_files=True\n",
    ")\n",
    "\n",
    "# Store summary metrics\n",
    "summary_metrics = {\n",
    "    'num_evaluated': results['num_evaluated'],\n",
    "    'num_with_prediction': results['num_with_prediction'],\n",
    "    'prediction_rate': results['prediction_rate'],\n",
    "    'execution_accuracy': results['execution_accuracy'],\n",
    "    'exact_match_accuracy': results['exact_match_accuracy'],\n",
    "    'semantic_equivalent_accuracy': results.get('semantic_equivalent_accuracy', 0.0),\n",
    "    'model': results['model'],\n",
    "    'optimization': results.get('optimization_used', 'conversational_schema_context')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "400ba70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_evaluated': 13,\n",
       " 'num_with_prediction': 12,\n",
       " 'prediction_rate': 0.9230769230769231,\n",
       " 'execution_accuracy': 0.08333333333333333,\n",
       " 'exact_match_accuracy': 0.0,\n",
       " 'semantic_equivalent_accuracy': 0.08333333333333333,\n",
       " 'model': {'model_name': 'meta-llama/Llama-3.3-70B-Instruct-Turbo-Free',\n",
       "  'model_type': 'together_ai',\n",
       "  'timestamp': '2025-06-08T00:43:22.382162',\n",
       "  'optimization': 'conversational_schema_context'},\n",
       " 'optimization': 'conversational_schema_context'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced10f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
