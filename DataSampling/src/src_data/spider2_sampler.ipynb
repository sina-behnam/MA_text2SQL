{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5834dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPIDER2_DATASET_PATH = '/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/Data/Spider2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eb250c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "def get_schemas_path(dataset_dir, method='json'):\n",
    "    \"\"\"\n",
    "    Get the path to the database schemas in the Spider2 dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_dir (str): The path to the Spider2 dataset directory.\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are database names and the values are lists of paths to the database schema files.\n",
    "    \"\"\"\n",
    "    if method == 'json':\n",
    "        database_files_path = glob.glob(dataset_dir + os.sep + '**' + os.sep + '*.json', recursive=True)\n",
    "    elif method == 'csv':\n",
    "        database_files_path = glob.glob(dataset_dir + os.sep + '**' + os.sep + '*.csv', recursive=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    db_schemas_path = {}\n",
    "\n",
    "    for file in database_files_path:\n",
    "        databases_path = file.split(dataset_dir)[-1]\n",
    "        # Getting the database name from the path to which database this files is belonging to\n",
    "        database_name = databases_path.split(os.sep)[1]\n",
    "        # create a list based on the database name in db_schemas_path\n",
    "        if database_name not in db_schemas_path:\n",
    "            db_schemas_path[database_name] = []\n",
    "        db_schemas_path[database_name].append(file)\n",
    "\n",
    "    return db_schemas_path\n",
    "\n",
    "def prepare_spider2_lite_files(dataset_dir, available_dbs=None,method=None):\n",
    "    \"\"\"\n",
    "    Prepare the Spider2 Lite dataset.\n",
    "    \"\"\"\n",
    "    db_dir = os.path.join(dataset_dir, 'spider2-lite','resource','databases')\n",
    "\n",
    "    db_paths = {}\n",
    "\n",
    "    for available_db in available_dbs:\n",
    "        \n",
    "        if available_db == 'snowflake':\n",
    "            snowflake_dir = os.path.join(db_dir, 'snowflake')\n",
    "\n",
    "            db_paths['snowflake'] = get_schemas_path(snowflake_dir,method=method)\n",
    "\n",
    "        elif available_db == 'sqlite':\n",
    "            sqlite_dir = os.path.join(db_dir, 'sqlite')\n",
    "\n",
    "            db_paths['sqlite'] = get_schemas_path(sqlite_dir,method=method)\n",
    "\n",
    "        elif available_db == 'bigquery':\n",
    "            bigquery_dir = os.path.join(db_dir, 'bigquery')\n",
    "\n",
    "            db_paths['bigquery'] = get_schemas_path(bigquery_dir,method=method)\n",
    "    \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown database type: {available_db}\")\n",
    "        \n",
    "    return db_paths\n",
    "\n",
    "\n",
    "def get_spider2_files(dataset_dir,category,available_dbs=None,method=None):\n",
    "    \"\"\"\n",
    "    Get the path to the Spider2 dataset directory.\n",
    "    \"\"\"\n",
    "    if category == 'snow':\n",
    "        raise NotImplementedError(\"Snowflake database preparation is not implemented yet.\")\n",
    "    elif category == 'lite':\n",
    "        return prepare_spider2_lite_files(dataset_dir, available_dbs,method)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown category: {category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a606ab9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snowflake</th>\n",
       "      <th>sqlite</th>\n",
       "      <th>bigquery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA</th>\n",
       "      <td>[/Users/sinabehnam/Desktop/Projects/Polito/The...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEO_OPENSTREETMAP_BOUNDARIES</th>\n",
       "      <td>[/Users/sinabehnam/Desktop/Projects/Polito/The...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ETHEREUM_BLOCKCHAIN</th>\n",
       "      <td>[/Users/sinabehnam/Desktop/Projects/Polito/The...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUSTIN</th>\n",
       "      <td>[/Users/sinabehnam/Desktop/Projects/Polito/The...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA_HG19_DATA_V0</th>\n",
       "      <td>[/Users/sinabehnam/Desktop/Projects/Polito/The...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cms_data</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[/Users/sinabehnam/Desktop/Projects/Polito/The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_york_noaa</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[/Users/sinabehnam/Desktop/Projects/Polito/The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid19_usa</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[/Users/sinabehnam/Desktop/Projects/Polito/The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlb</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[/Users/sinabehnam/Desktop/Projects/Polito/The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>san_francisco_plus</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[/Users/sinabehnam/Desktop/Projects/Polito/The...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                      snowflake  \\\n",
       "TCGA                          [/Users/sinabehnam/Desktop/Projects/Polito/The...   \n",
       "GEO_OPENSTREETMAP_BOUNDARIES  [/Users/sinabehnam/Desktop/Projects/Polito/The...   \n",
       "ETHEREUM_BLOCKCHAIN           [/Users/sinabehnam/Desktop/Projects/Polito/The...   \n",
       "AUSTIN                        [/Users/sinabehnam/Desktop/Projects/Polito/The...   \n",
       "TCGA_HG19_DATA_V0             [/Users/sinabehnam/Desktop/Projects/Polito/The...   \n",
       "...                                                                         ...   \n",
       "cms_data                                                                    NaN   \n",
       "new_york_noaa                                                               NaN   \n",
       "covid19_usa                                                                 NaN   \n",
       "mlb                                                                         NaN   \n",
       "san_francisco_plus                                                          NaN   \n",
       "\n",
       "                             sqlite  \\\n",
       "TCGA                            NaN   \n",
       "GEO_OPENSTREETMAP_BOUNDARIES    NaN   \n",
       "ETHEREUM_BLOCKCHAIN             NaN   \n",
       "AUSTIN                          NaN   \n",
       "TCGA_HG19_DATA_V0               NaN   \n",
       "...                             ...   \n",
       "cms_data                        NaN   \n",
       "new_york_noaa                   NaN   \n",
       "covid19_usa                     NaN   \n",
       "mlb                             NaN   \n",
       "san_francisco_plus              NaN   \n",
       "\n",
       "                                                                       bigquery  \n",
       "TCGA                                                                        NaN  \n",
       "GEO_OPENSTREETMAP_BOUNDARIES                                                NaN  \n",
       "ETHEREUM_BLOCKCHAIN                                                         NaN  \n",
       "AUSTIN                                                                      NaN  \n",
       "TCGA_HG19_DATA_V0                                                           NaN  \n",
       "...                                                                         ...  \n",
       "cms_data                      [/Users/sinabehnam/Desktop/Projects/Polito/The...  \n",
       "new_york_noaa                 [/Users/sinabehnam/Desktop/Projects/Polito/The...  \n",
       "covid19_usa                   [/Users/sinabehnam/Desktop/Projects/Polito/The...  \n",
       "mlb                           [/Users/sinabehnam/Desktop/Projects/Polito/The...  \n",
       "san_francisco_plus            [/Users/sinabehnam/Desktop/Projects/Polito/The...  \n",
       "\n",
       "[158 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "spider2_schemas_path = get_spider2_files(SPIDER2_DATASET_PATH,\n",
    "                                        category='lite',\n",
    "                                        available_dbs=['snowflake', 'sqlite','bigquery'],\n",
    "                                        method='csv')\n",
    "\n",
    "schemas_path_df = pd.DataFrame.from_dict(spider2_schemas_path, orient='index').T\n",
    "\n",
    "schemas_path_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ff9451e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/Data/Spider2/spider2-lite/resource/databases/snowflake/AUSTIN/AUSTIN_INCIDENTS/DDL.csv',\n",
       " '/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/Data/Spider2/spider2-lite/resource/databases/snowflake/AUSTIN/AUSTIN_BIKESHARE/DDL.csv',\n",
       " '/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/Data/Spider2/spider2-lite/resource/databases/snowflake/AUSTIN/AUSTIN_311/DDL.csv',\n",
       " '/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/Data/Spider2/spider2-lite/resource/databases/snowflake/AUSTIN/AUSTIN_CRIME/DDL.csv',\n",
       " '/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/Data/Spider2/spider2-lite/resource/databases/snowflake/AUSTIN/AUSTIN_WASTE/DDL.csv']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemas_path_df['snowflake']['AUSTIN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2a91c880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TCGA', 'GEO_OPENSTREETMAP_BOUNDARIES', 'ETHEREUM_BLOCKCHAIN', 'AUSTIN',\n",
       "       'TCGA_HG19_DATA_V0', 'IDC', 'CRYPTO', 'GENOMICS_CANNABIS', 'PYPI',\n",
       "       'GEO_OPENSTREETMAP',\n",
       "       ...\n",
       "       'new_york_geo', 'eclipse_megamovie', 'noaa_ports',\n",
       "       'census_bureau_international', 'new_york_ghcn', 'cms_data',\n",
       "       'new_york_noaa', 'covid19_usa', 'mlb', 'san_francisco_plus'],\n",
       "      dtype='object', length=158)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemas_path_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "8a4ea3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO,)\n",
    "logger = logging.getLogger(__name__)\n",
    "# disable the logger\n",
    "# logger.setLevel(logging.CRITICAL)\n",
    "# enable the logger\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Read a JSON file and return its content.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))  \n",
    "    return data\n",
    "\n",
    "def sync_question_key_name(instance):\n",
    "        \"\"\"\n",
    "        Ensure the question key name is consistent across instances.\n",
    "        This method checks if the instance has a 'question' key and renames it if necessary.\n",
    "        If the instance has a different key name for the question (e.g., 'instruction' or 'query'), it renames it to 'question'.\n",
    "        If no valid key is found, it raises a KeyError.\n",
    "        Args:\n",
    "            instance: Dictionary containing the original instance data\n",
    "        Returns:\n",
    "            Updated instance dictionary with the correct question key name\n",
    "        \"\"\"\n",
    "        if 'question' in instance:\n",
    "            return instance\n",
    "        elif 'instruction' in instance:\n",
    "            instance['question'] = instance.pop('instruction')\n",
    "        elif 'query' in instance:\n",
    "            instance['question'] = instance.pop('query')\n",
    "        else:\n",
    "            # If no known key exists, raise an error\n",
    "            raise KeyError(\"No valid question key found in the instance.\")\n",
    "        return instance   \n",
    "\n",
    "def get_sql_query_per_instance(instance_id,quires_dir):\n",
    "        \"\"\"\n",
    "        Get the SQL query for a specific instance ID.\n",
    "\n",
    "        Args:\n",
    "            instance_id: ID of the instance\n",
    "        Returns:\n",
    "            SQL query string or None if not found  \n",
    "        \"\"\"\n",
    "        if quires_dir is not None:\n",
    "            # Get the path to the SQL file\n",
    "            sql_file_path = os.path.join(quires_dir, f\"{instance_id}.sql\")\n",
    "            if not os.path.exists(sql_file_path):\n",
    "                logger.debug(f\"SQL file not found for instance {instance_id}: {sql_file_path}\")\n",
    "                return None\n",
    "\n",
    "            # Read the SQL query from the file\n",
    "            with open(sql_file_path, 'r', encoding='utf-8') as f:\n",
    "                sql_query = f.read()\n",
    "            \n",
    "            return sql_query.strip()\n",
    "        else:\n",
    "            logger.error(f\"No queries directory specified for instance {instance_id}\")\n",
    "            raise ValueError(f\"No queries directory specified for instance {instance_id}\")\n",
    "\n",
    "def get_external_knowledge_instance(external_knowledge_file: str,external_know_dir: str):\n",
    "        \"\"\"\n",
    "        Get external knowledge for a specific instance.\n",
    "\n",
    "        Args:\n",
    "            external_knowledge_file: Name of the external knowledge file.\n",
    "            external_know_dir: Directory containing the external knowledge files.\n",
    "        \n",
    "        Returns:\n",
    "            External knowledge string or None if not found\n",
    "        \"\"\"\n",
    "\n",
    "        if external_knowledge_file is not None and (external_knowledge_file != '' or external_knowledge_file != []):\n",
    "            # Load the external knowledge from the directory\n",
    "            external_knowledge_path = os.path.join(external_know_dir, external_knowledge_file)\n",
    "            if not os.path.exists(external_knowledge_path):\n",
    "                raise FileNotFoundError(f\"External knowledge file not found: {external_knowledge_path}\")\n",
    "            # IT is a .md file \n",
    "            with open(external_knowledge_path, 'r', encoding='utf-8') as f:\n",
    "                external_knowledge_data = f.read()\n",
    "            \n",
    "            return external_knowledge_data.strip()\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "def get_database_schema(database_name : str, schema_paths_df : pd.DataFrame, available_dbs: list = None):\n",
    "    \"\"\"\n",
    "    Check if the database schema exists for a given database name.\n",
    "    This function checks if the database name exists in the provided database schemas path dictionary.\n",
    "\n",
    "    Args:\n",
    "        database_name: Name of the database\n",
    "        schema_paths_df: Dataframe containing the database schemas paths\n",
    "        available_dbs: List of available databases to check against\n",
    "    Returns:\n",
    "        tuple: (database_category, schema_path) if the database schema exists,\n",
    "                None if the database schema does not exist.\n",
    "    \"\"\"\n",
    "    if available_dbs is None:\n",
    "        available_dbs = schema_paths_df.columns.tolist()\n",
    "\n",
    "    if database_name not in schema_paths_df.index:\n",
    "        logger.warning(f\"Database name {database_name} not found in the schema paths dataframe.\")\n",
    "        return None\n",
    "    \n",
    "    schema_paths = schemas_path_df.loc[database_name, available_dbs].dropna().to_dict()\n",
    "\n",
    "    # ! Here the database name may not belong to any of the available databases, so the schema_paths will be empty\n",
    "    if not schema_paths:\n",
    "        return None\n",
    "    \n",
    "    # ! Getting ONLY the first key of the schema_paths dictionary, meaning that we are assuming that the a database name can only belong to one category of \n",
    "    # ! database, e.g. sqlite, bigquery, snowflake\n",
    "    if len(schema_paths) > 1:\n",
    "        logger.warning(f\"Multiple database categories found for database name: {database_name}. Using the first one found.\")\n",
    "\n",
    "    database_cat = list(schema_paths.keys())[0] \n",
    "\n",
    "    return database_cat, schema_paths[database_cat]\n",
    "    \n",
    "\n",
    "def get_sqlite_db_file_path(sqlite_file_dir, db_name):\n",
    "    \"\"\"\n",
    "    Check if the SQLite database file by the name of the database name exists in the directory.\n",
    "    Then it returns the path to the SQLite database file.\n",
    "\n",
    "    Args:\n",
    "        sqlite_file_dir: Directory containing the SQLite database files\n",
    "        db_name: Name of the database \n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the SQLite database file\n",
    "    \"\"\"\n",
    "    # Check if the SQLite database file exists in the directory\n",
    "    sqlite_db_file_path = os.path.join(sqlite_file_dir, db_name + '.sqlite')\n",
    "    if os.path.exists(sqlite_db_file_path):\n",
    "        return sqlite_db_file_path\n",
    "    return None\n",
    "\n",
    "def standarize_spider2_instance(instance, count):\n",
    "\n",
    "    new_instance = {\n",
    "        'id': count,\n",
    "        'original_instance_id': instance.get('instance_id', None),\n",
    "        'dataset' : instance.get('dataset', None),\n",
    "        'question': instance.get('question', None),\n",
    "        'sql': instance.get('sql', None),\n",
    "        'database': instance.get('database', None),\n",
    "        'schemas': instance.get('schemas', None),\n",
    "        'evidence': instance.get('evidence', None)\n",
    "    }\n",
    "    return new_instance\n",
    "\n",
    "def load_data(data_file_path,\n",
    "                limit,\n",
    "                queries_dir=None,\n",
    "                external_knowledge_dir=None,\n",
    "                schemas_path_df=None,\n",
    "                available_dbs=None,\n",
    "                sqlites_file_dir=None,\n",
    "                dataset_type='lite'\n",
    "              ):\n",
    "        \"\"\"\n",
    "        Load the Spider2 dataset.\n",
    "\n",
    "        Returns:\n",
    "            List of examples\n",
    "        \"\"\"\n",
    "        # check the exisitace of the file\n",
    "        if not os.path.exists(data_file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {data_file_path}\")\n",
    "        \n",
    "        # Read the JSON file\n",
    "        data = read_json_file(data_file_path)\n",
    "            \n",
    "        # Apply limit if specified\n",
    "        if limit is not None:\n",
    "            data = data[:limit]\n",
    "\n",
    "        results_data =[]\n",
    "        \n",
    "        for count,value in enumerate(data):\n",
    "\n",
    "            instance = value.copy()  # Create a copy of the instance to avoid modifying the original data\n",
    "\n",
    "            instance_id = instance.get('instance_id')\n",
    "\n",
    "            if instance_id is None:\n",
    "                logger.warning(\"No instance ID found for example\")\n",
    "                continue\n",
    "\n",
    "            # Check if the database exists for the given instance\n",
    "            database_name = instance.get('db')\n",
    "            if database_name is None:\n",
    "                logger.warning(f\"No database name found for instance {instance_id}\")\n",
    "                continue\n",
    "\n",
    "            # Ensure the question key name is consistent\n",
    "            instance = sync_question_key_name(instance)\n",
    "\n",
    "            query = get_sql_query_per_instance(instance_id, queries_dir)\n",
    "\n",
    "            if query is None:\n",
    "                continue\n",
    "            \n",
    "            # Add the SQL query to the instance\n",
    "            instance['sql'] = query\n",
    "\n",
    "            external_knowledge = instance.get('external_knowledge', None)\n",
    "\n",
    "            instance['evidence'] = get_external_knowledge_instance(external_knowledge, external_knowledge_dir)\n",
    "\n",
    "            # Check if the database schema exists for the given instance\n",
    "            db_schemas = get_database_schema(database_name, schemas_path_df, available_dbs)\n",
    "            if db_schemas is None:\n",
    "                continue\n",
    "\n",
    "            database_cat, db_schemas_path = db_schemas\n",
    "\n",
    "            if database_cat == 'sqlite':\n",
    "                \n",
    "                sqlite_path = get_sqlite_db_file_path(sqlites_file_dir, database_name)\n",
    "                \n",
    "                if not sqlite_path:\n",
    "                    logger.warning(f\"SQLite database file not found for instance {instance_id} with database name {database_name}\")\n",
    "                    continue\n",
    "\n",
    "                instance['database'] = {\n",
    "                    'name': database_name,\n",
    "                    'path': sqlite_path,\n",
    "                    'type': database_cat\n",
    "                }\n",
    "            elif database_cat == 'snowflake':\n",
    "                instance['database'] = {\n",
    "                    'name': database_name,\n",
    "                    'path': 'Call the snowflake API to get the database',\n",
    "                    'type': database_cat\n",
    "                }\n",
    "            elif database_cat == 'bigquery':\n",
    "                instance['database'] = {\n",
    "                    'name': database_name,\n",
    "                    'path': 'Call the snowflake API to get the database',\n",
    "                    'type': database_cat\n",
    "                }\n",
    "            else:\n",
    "                logger.warning(f\"Unknown database category for instance {instance_id} with database name {database_name} with category {database_cat}\")\n",
    "                continue\n",
    "\n",
    "            instance['schemas'] = []\n",
    "            for db_schema in db_schemas_path:\n",
    "                instance['schemas'].append({\n",
    "                    'name': db_schema.split(os.sep)[-2],  # Extract the database name from the path\n",
    "                    'path': db_schema\n",
    "                })\n",
    "\n",
    "            instance['dataset'] = f'spider2-{dataset_type}'\n",
    "\n",
    "            # Standardize the instance and append it to the results\n",
    "            results_data.append(standarize_spider2_instance(instance, count))\n",
    "\n",
    "        logger.info(f\"Loaded {len(results_data)} examples from Spider2 dataset\")\n",
    "        \n",
    "        logger.info(\"IF the number of loaded Data are less than what you expected, is because of the missing SQL queries in GOLD Directory\")\n",
    "        return results_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a83528c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(SPIDER2_DATASET_PATH, 'spider2-lite', 'spider2-lite.jsonl')\n",
    "# Data/Spider2/spider2-lite/evaluation_suite/gold/sql\n",
    "queries_dir = os.path.join(SPIDER2_DATASET_PATH, 'spider2-lite', 'evaluation_suite', 'gold', 'sql')\n",
    "# Data/Spider2/spider2-lite/resource/documents\n",
    "external_knowledge_dir = os.path.join(SPIDER2_DATASET_PATH, 'spider2-lite', 'resource', 'documents')\n",
    "# Data/Spider2/spider2-lite/resource/databases/spider2-localdb\n",
    "sqlite_file_dir = os.path.join(SPIDER2_DATASET_PATH, 'spider2-lite', 'resource', 'databases', 'spider2-localdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "47eb7547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Database name Db-IMDB not found in the schema paths dataframe.\n",
      "WARNING:__main__:Database name sqlite-sakila not found in the schema paths dataframe.\n",
      "WARNING:__main__:Database name sqlite-sakila not found in the schema paths dataframe.\n",
      "INFO:__main__:Loaded 105 examples from Spider2 dataset\n",
      "INFO:__main__:IF the number of loaded Data are less than what you expected, is because of the missing SQL queries in GOLD Directory\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "resulted_data = load_data(data_path,\n",
    "                limit=None,\n",
    "                queries_dir=queries_dir,\n",
    "                external_knowledge_dir=external_knowledge_dir,\n",
    "                schemas_path_df=schemas_path_df,\n",
    "                available_dbs=['sqlite', 'snowflake'],\n",
    "                sqlites_file_dir=sqlite_file_dir,\n",
    "                dataset_type='lite'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3aec3ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 71,\n",
       " 'original_instance_id': 'sf_bq236',\n",
       " 'dataset': 'spider2-lite',\n",
       " 'question': \"What are the top 5 zip codes of the areas in the United States that have experienced the most hail storm events in the past 10 years? Don't use data from hail reports table.\",\n",
       " 'sql': 'SELECT\\n  CONCAT(\"city\", \\', \\', \"state_name\") AS \"city\",\\n  \"zip_code\",\\n  COUNT(\"event_id\") AS \"count_storms\"\\nFROM (\\n    SELECT *\\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2014\\n    UNION ALL\\n    SELECT *\\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2015\\n    UNION ALL\\n    SELECT *\\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2016\\n    UNION ALL\\n    SELECT *\\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2017\\n    UNION ALL\\n    SELECT *\\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2018\\n    UNION ALL\\n    SELECT *\\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2019\\n    UNION ALL\\n    SELECT *\\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2020\\n    UNION ALL\\n    SELECT *\\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2021\\n    UNION ALL\\n    SELECT *\\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2022\\n    UNION ALL\\n    SELECT *\\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2023\\n    UNION ALL\\n    SELECT *\\n    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2024\\n) AS storms\\nJOIN NOAA_DATA_PLUS.GEO_US_BOUNDARIES.ZIP_CODES\\n  ON ST_WITHIN(ST_GEOGFROMWKB(storms.\"event_point\"), ST_GEOGFROMWKB(\"zip_code_geom\"))\\nWHERE\\n   LOWER(storms.\"event_type\") = \\'hail\\'\\nGROUP BY\\n  \"zip_code\", \\n  \"city\", \\n  \"state_name\"\\nORDER BY\\n  \"count_storms\" DESC\\nLIMIT 5;',\n",
       " 'database': {'name': 'NOAA_DATA_PLUS',\n",
       "  'path': 'Call the snowflake API to get the database',\n",
       "  'type': 'snowflake'},\n",
       " 'schemas': [{'name': 'NOAA_PRELIMINARY_SEVERE_STORMS',\n",
       "   'path': '/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/Data/Spider2/spider2-lite/resource/databases/snowflake/NOAA_DATA_PLUS/NOAA_PRELIMINARY_SEVERE_STORMS/DDL.csv'},\n",
       "  {'name': 'GEO_US_BOUNDARIES',\n",
       "   'path': '/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/Data/Spider2/spider2-lite/resource/databases/snowflake/NOAA_DATA_PLUS/GEO_US_BOUNDARIES/DDL.csv'},\n",
       "  {'name': 'NOAA_PASSIVE_ACOUSTIC_INDEX',\n",
       "   'path': '/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/Data/Spider2/spider2-lite/resource/databases/snowflake/NOAA_DATA_PLUS/NOAA_PASSIVE_ACOUSTIC_INDEX/DDL.csv'},\n",
       "  {'name': 'NOAA_ICOADS',\n",
       "   'path': '/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/Data/Spider2/spider2-lite/resource/databases/snowflake/NOAA_DATA_PLUS/NOAA_ICOADS/DDL.csv'},\n",
       "  {'name': 'NOAA_HISTORIC_SEVERE_STORMS',\n",
       "   'path': '/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/Data/Spider2/spider2-lite/resource/databases/snowflake/NOAA_DATA_PLUS/NOAA_HISTORIC_SEVERE_STORMS/DDL.csv'},\n",
       "  {'name': 'NOAA_GSOD',\n",
       "   'path': '/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/Data/Spider2/spider2-lite/resource/databases/snowflake/NOAA_DATA_PLUS/NOAA_GSOD/DDL.csv'},\n",
       "  {'name': 'NOAA_PIFSC_METADATA',\n",
       "   'path': '/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/Data/Spider2/spider2-lite/resource/databases/snowflake/NOAA_DATA_PLUS/NOAA_PIFSC_METADATA/DDL.csv'},\n",
       "  {'name': 'NOAA_SIGNIFICANT_EARTHQUAKES',\n",
       "   'path': '/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/Data/Spider2/spider2-lite/resource/databases/snowflake/NOAA_DATA_PLUS/NOAA_SIGNIFICANT_EARTHQUAKES/DDL.csv'},\n",
       "  {'name': 'NOAA_TSUNAMI',\n",
       "   'path': '/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/Data/Spider2/spider2-lite/resource/databases/snowflake/NOAA_DATA_PLUS/NOAA_TSUNAMI/DDL.csv'},\n",
       "  {'name': 'NOAA_PASSIVE_BIOACOUSTIC',\n",
       "   'path': '/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/Data/Spider2/spider2-lite/resource/databases/snowflake/NOAA_DATA_PLUS/NOAA_PASSIVE_BIOACOUSTIC/DDL.csv'},\n",
       "  {'name': 'NOAA_HURRICANES',\n",
       "   'path': '/Users/sinabehnam/Desktop/Projects/Polito/Thesis/MA_text2SQL/Data/Spider2/spider2-lite/resource/databases/snowflake/NOAA_DATA_PLUS/NOAA_HURRICANES/DDL.csv'}],\n",
       " 'evidence': \"Categories: Geospatial functions\\n\\n\\n## ST_WITHIN\\n\\nReturns true if the first geospatial object is fully contained by the second geospatial object. In other words:\\n\\nThe first GEOGRAPHY object g1 is fully contained by the second GEOGRAPHY object g2.\\nThe first GEOMETRY object g1 is fully contained by the second GEOMETRY object g2.\\n\\nCalling ST_WITHIN(g1, g2) is equivalent to calling ST_CONTAINS(g2, g1).\\nAlthough ST_COVEREDBY and ST_WITHIN might seem similar, the two functions have subtle differences. For details on the differences between “covered by” and “within”, see the Dimensionally Extended 9-Intersection Model (DE-9IM).\\n\\nNote This function does not support using a GeometryCollection or FeatureCollection as input values.\\n\\nTip You can use the search optimization service to improve the performance of queries that call this function.\\nFor details, see Search Optimization Service.\\n\\nSee also:ST_CONTAINS , ST_COVEREDBY\\n\\n\\n## Syntax\\n\\nST_WITHIN( <geography_expression_1> , <geography_expression_2> )\\n\\nST_WITHIN( <geometry_expression_1> , <geometry_expression_2> )\\n\\n\\n## Arguments\\n\\n\\ngeography_expression_1A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.\\n\\ngeography_expression_2A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.\\n\\ngeometry_expression_1A GEOMETRY object that is not a GeometryCollection or FeatureCollection.\\n\\ngeometry_expression_2A GEOMETRY object that is not a GeometryCollection or FeatureCollection.\\n\\n\\n## Returns\\n\\nBOOLEAN.\\n\\n## Examples\\n\\n\\n## GEOGRAPHY examples\\n\\nThis shows a simple use of the ST_WITHIN function:\\n\\ncreate table geospatial_table_01 (g1 GEOGRAPHY, g2 GEOGRAPHY);\\ninsert into geospatial_table_01 (g1, g2) values \\n    ('POLYGON((0 0, 3 0, 3 3, 0 3, 0 0))', 'POLYGON((1 1, 2 1, 2 2, 1 2, 1 1))');\\n\\nCopy SELECT ST_WITHIN(g1, g2) \\n    FROM geospatial_table_01;\\n+-------------------+\\n| ST_WITHIN(G1, G2) |\\n|-------------------|\\n| False             |\\n+-------------------+\"}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_sample_index = random.randint(0, len(resulted_data) - 1)\n",
    "resulted_data[random_sample_index]  # Example to access a random sample from the resulted data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
